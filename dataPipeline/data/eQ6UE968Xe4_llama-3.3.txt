00:00:00.960 - 00:00:30.000 | Classification task introduction
we have seen earlier the regression problem where we have effectively to model a conditional probability distribution at the output of our predictor now we actually switching to a new task it's a classification task where again we will solve this task using The Vaping block diagram again we will need to model our predictor with a conditional probability distribution but in classification our Target variables are distinct and discrete random variables rather than continues

00:00:30.840 - 00:01:26.400 | Radar problem use case
so in this kind of setting I'll motivate the classification task with a simple use case is actually going to be called the radar problem and we'll that's what we will start with next in this setting the use case the application we will see is a well-known application back in the Second World War the Battle of England was W primarily from the erection of these towers this was actually called the radar Towers whose job is was to transmit a signal towards the France where from France the Nazi airplanes were coming in to bomb London

00:01:27.400 - 00:02:19.879 | Radar system description
and the every time that the this waveform was impinging into some large object on the sky like a plane it was returning back into what we call the radar receiver and there was kind of a human operator over there in on the on each kind of Tower with an access to a kind of a telephone device over there and every time that there was a strong return of a strong signal that was received in the radar kind of a receiver antenna it was he was calling London and millions of people were well the sirens were sounding and millions of people were actually running to the tubes stations to tube stations to save their lives

00:02:20.480 - 00:03:06.680 | Modern car application
that was the application and this application is evidently present in any modern car today that has exactly the same ability to have Raiders located in the let's say in the front of the car to send exactly the same signals and every time that you have a feature in your car that it's automatic what is called automatic distance keeping to the vehicle in front of you that is exactly the same thing it was returning back that Reflection from the car in front and controller is actually trying to keep the velocities between your car and the car in front of you constant and therefore maintain a desired distance between the two cars

00:03:07.560 - 00:04:06.680 | Time plot of signal strength
but anyway I would motivate it with this sort of application which is the World War II application because also back then a lot of the terminology that we will cover today it was also first invented and just to see exactly what is actually happening here we have a some kind of a time plot of a quantity that will be calling X and it will be designating for us the strength of or received signal strength or power and very simplistically we have some you know some fluctuation received power when we have a return a strong return from a plane and in nights where we don't the attacks were usually during the night that when we have no return we still have some fluctuating signal power much smaller than that

00:04:08.159 - 00:04:59.840 | Threshold value
and okay our receiver is going to have just one knob that knob is going to be called the threshold will symbolize it with W it's a scalar value that it is anything that exceeds it is going to let's assume that this threshold value is set to this kind of level everything that exceeds it is going to be called predicted that is well not predicted is we're going to sound the alarm and anything that it is not exceeding it we are going to not alarm anyone alert anyone and we will call that the no attack case

00:05:00.479 - 00:05:44.160 | Binary classification problem
so our problem is a binary classification problem what we call binary because our now Target variable as compared to that regression can only take two values one we will call this the so-called positive condition the attack is going on and zero we will be calling this the So-Cal negative condition and our job here now that we are in the 2020s we are going to solve this problem of determining the value of that kind of threshold using kind of machine learning approaches

00:05:44.880 - 00:06:28.280 | Data collection
so we have what we will do is we will task a military person over here to sit and observe what is really happening every single day so first night records the signal strength X1 and Records also whether attack has happened or not X2 the same way and so on if they survive we will keep them if they not we have to replace them with somebody else so XM ym that's our let's say we have aggregated M examples of what is has happened at our receiver

00:06:29.160 - 00:07:04.319 | Problem statement
and as before we progress into specifying what is really the problem statement I just want to U convey some kind of an intuitive fashion of what is happening so this was the soal attack signal strength and this was the no attack signal strength the sort of time series of all these kind of observations night after night and we have the problem to solve is to come up with an optimal value for the threshold

00:07:05.160 - 00:08:02.039 | Threshold importance
you can understand that the threshold is quite critical in determining whether how the system operates if we set the threshold too high this means that we will be missing quite a lot of attacks and evidently people will die if we set the threshold too low this means that we will be waking up everyone unnecessarily the first night they will believe us and you know return to their beds disappointed somehow or relieved the second night they will still believe us they will go down to the tube but after the third night they will stop believing us and when a real ATT happens people will also die so the system will lose complete credibility

00:08:02.840 - 00:08:31.120 | Predictor output
so our predictor the Y hat that we have to predict at this moment in time we will treat this as a one and zero later will become a number between zero and one is also going to return our prediction predictor is going also to return one or zero for the positive and negative condition respectively

00:08:31.440 - 00:09:22.399 | Probability distributions
so what we will now is we draw two probability distributions and let me just draw this probability distribution let's say this so this is a probability distribution of X we have here on the x- axis our signal strength and one of them is going to be called the probability of X and when no attack is happening and this is the probability of X when the attack is happening

00:09:23.399 - 00:09:52.959 | Histograms
and as you can see these two prob distributions can be easily obtained U as histograms by just going back to our data tape the data set that we have created and recorded and select all the rows where the Y Target variable is zero and obtain this histogram and visit all the corresponding rows where the Y is equal to one and plot this corresponding histogram

00:09:53.600 - 00:10:25.480 | Overlap between histograms
and as you probably notice from the shape of these histograms we have not really made any assumption with respect to U gaussianity or nothing like that they are definitely plotted as non- gausian type of probability distributions and the moment I have and also another sort of evident that thing that is actually going on with this problem is that there is a very strong overlap between the two histograms when we have the signal strength between no attacks and attacks

00:10:25.640 - 00:11:15.880 | Confusion Matrix
the overlap means that since there's no absolutely clear separation between the two prob distributions we are always going to make some form of mistake we always going to have mistakes and in fact we can clearly distinguish four conditions and we will tabulate them with what we call the confusion Matrix on the one axis of the confusion Matrix we'll be assigning this axis to the so-call ground truth and the other to the prediction the yut and when the Y hat is positive and negative and this positive and negative agrees to the with the ground truth then we will be calling this correspondingly true positive and true negative

00:11:16.079 - 00:12:02.160 | False positives and negatives
in fact it's actually quite common to write first the letter the second letter as a pneumonic rule write first the second letter and put a word the letter T in front every time the that you have agreement with ground truth in the case where you predict there is an attack going on but you are wrong you are prepending it with the letter F stands for false so we have false positive here and here we have also false negative we are predicting negative but we are wrong and therefore we have the so-called false negative events

00:12:02.440 - 00:12:48.360 | Optimal threshold
so the moment I have specified the threshold location over here w we have split the region into two parts the first region is called r0 and the second region is called R1 and now that we have this region names I think the region names are also intuitively kind of understood because this zero index here corresponds to the case where we declare anything as we said below the threshold W is there's no attack we are predicting no attack

00:12:48.760 - 00:13:50.279 | Probability of mistakes
so what I'm going to do now is I'm going to declare that this probability the first probability over here is equal to the probability that I am making a prediction such as my X belongs to the region r0 when in fact the ground truth is one so I converted the Y is equal to zero to all the events which are to the left of w so all the events where X belongs to this kind of region are zero and the do the same belong to the region R1 when Y is equal to Z all events of X greater than W in other words all X's which are belonging to the region R1

00:13:50.440 - 00:14:55.680 | Optimizing W towards W star
so actually we can write that sort of optimizing the W towards W star will be using the well-known to us stochastic gr descent algorithm that minimizes the probability of mistakes probability of making a mistake so the misclassification error or also misclassification error eight so this will be done now that we have some kind of a visual motivation of what we're trying to achieve here we now need to understand how we can also motivate in the next discussion an objective function and that it is going to be suitable for our problem here which is the classification problem in a similar way as we have done with the U earlier loss function we have used initially was called mean square error and then it was also called cross entropy so we'll do that next the come up with this objective function before we go and discuss that review the so-called classification metrics

00:14:56.040 - 00:17:08.079 | Classification metrics
the classification metrics that there a couple of classification metrics will be of interest to us they will be entirely based of course on the previously described confusion Matrix and the first matric I want to address is called the true positive rate the second metric is well the Dr postive rate is comes with many names and many of them have been sort of originating from various kind of domains electrical engineering computer science and others so in computer science this also is called recall in electrical engineering this is also called probability of detection and many other domains quote it as it's one and the same thing and I just want to mention all of them just in case you come up with come across one of the of the many so this is the ratio between true positive and true positive plus false negative so this is a ratio that is definitely going to be of concern to us and of interest to us every time we have to evaluate a classifier and the second metrix that I want to quote and have some discussion about those metrics a bit later is a so-called precision and this Precision is another ratio of true positive IDE by true positive plus false positive

00:17:08.919 - 00:18:30.840 | Precision and recall
and if you follow the this video where we have plotted these histograms in the binary classifier when the so-called the radar problem and you probably understood the tradeoff that exists between false positives and false negatives as we were moving in fact the as we were moving the value of the threshold w we were changing the areas under those two histograms and of course here we were trading all false positive or false negatives in our attempt to find this optimal kind of W in a very similar way we can actually claim that now that we have the those metrics the tradeoff between false positives and false negatives is evident over here U in the following trade off so let me write it this down so we can say that because as W changes there is a tradeoff between false positives and false negatives we can actually claim that there is a between recall and precision because recall and precision everything is exactly the same in terms of numerator and portion of the denominator but only the false POS and false negatives are present there

00:18:31.480 - 00:20:10.039 | Receiver operating characteristic
so this is actually an important tradeoff that will be of great interest to us as we will always finding ourselves making that kind of tradeoff for classif classification architectures we will be designing soon the other metric it's not really a different metric but it's a way to present performance metrics classification metrix is this what we call the So-Cal receiver operating characteristic and we actually call it receiver operating characteristic from those days in the 40s when they were deploying this kind of Radars and I will describe it as the curve that we can plot by changing the threshold W in the x-axis over here it is the false positive rate also known as a false alarm from those days the probab ility of the probability of false alarm PFA and the Y AIS is called recall evidently the same thing as a true positive rate and definitely we have a probability of false positive rate that goes from one from 0 to one and the probability of recall true positive rate that goes again from 0 to one because there are probabilities and therefore we'll find this Cur of constraint by those values

00:20:10.280 - 00:21:13.640 | Binary cross-entropy loss function
and we recognize the functional form of the so-call binary crossentropy loss function which is for spe specifically for our binary classification problem and we will be motivating this by recognizing that in regression we had a p mod which actually was gausian in binary classification we going to need to actually have a probabilistic model a probability distribution which is really appropriate for our discrete random variables that are the are wise so our form of myp model of let's say y given an X comma W this y over here is definitely a discrete and in fact binary so I have seen already in the discussion of the entropy video coing and I know that at that point I have quoted beri distribution as the appropriate propability distribution for our model and the beri distribution let me write it with all words over here is given as y hat to the power of Y 1 - y the^ of 1 - y

00:21:17.760 - 00:22:43.200 | Beri distribution and posterior probability
let's spend some time kind of understanding this if my ground truth is one this P model of Y had of Y sorry given X comma W is simply y because the Y is one so this is zero and therefore this term hole is one so the only thing that remains is y and in fact this is a very important in a sense that from now on in binary classification all I need to produce at the output is this a single floating Point number between zero and one so it's a probability so it's going to be zero between Z and one for sure I'll be calling this probability when Y is equal to 1 the probability of Y is equal to 1 given X comma w in fact I can write it as a the model and this is actually going to be called the posterior probability and we recognize the term posterior already we recognized it when we had this probability review lecture and in that kind of video we have actually seen that posterior it means after we get to observe the AIS our data what can we say about our Target variable Y in this case

00:22:43.760 - 00:24:19.520 | Binary cross-entropy calculation
so the BET data High distribution is present in here so no changes what we have seen earlier it's just a different type of data and I'm actually interested to go ahead and calculate this term over here and this term can be trivially calculated as log of my P model is why to^ of y * 1 - y the 1 - y or log y to the^ of Y plus log of 1 - y to the^ of 1 - Y and this is U of course making the use of the log of the product is the summation of the logs identity and this is uses another identity this is y log of y hat + 1 - y log of 1 - y hat and for those who remember the discussion of the entropy we have seen a sort of identical kind of term in the presentation of the binary kind of entropy graph the coin tossing during the coin tossing experiment

00:24:20.520 - 00:26:06.520 | Block diagram and loss function
so now we have everything that we need to draw a block diagram and this block diagram is applicable in fact to all sorts of predictors and we have already seen regression now in classification so let me just throw the predictor as this box over here we have seen that in the regression setting a linear model a DOT product in other words between the parameter vector and features and we also seeing here a classifier but we have not really discussed yet the functional form of what the classifier will actually be but whatever this classifier is going to so this is either for aggression or classification and input X is going to come in general it has many dimensions a y height is going to be produced at the output and over here we're going to have the let's assume that this is now for U I mean let's call it a loss function and the loss function will be the mean square error or the cross entropy in fact the we will show that the cross entropy is able to accommodate both mean square error as well and as well also the binary concenter

00:26:06.760 - 00:26:59.320 | Binary cross entropy
we have seen a sort of identical kind of term in the presentation of the binary kind of entropy graph the coin tossing during the coin tossing experiment so let's plug this in into this formula and at the same time we will replace the expectation with a sample mean so from these two we can conclude that the cross entropy the binary cross entropy so I'm just going to put a capital B in front to distinguish for the cross entropy for multiclass problems it is y hat comma y minus 1 / M summation from I is equal to 1 to m Yi log of Yi hat plus 1 - y I log of 1 - y i hat

00:27:00.039 - 00:27:47.120 | Loss function evaluation
definitely this is a very easy to evaluate this is definitely a scaler which is indicates to me whether how well am I doing and this is the loss function that is going to govern from now on our binary classification problem and we are need to use it as an objective function and when we minimize it equivalently we will be reducing the probabilistic distance between the P data hack that is present in my data set that is actually given to me and the P model which as we discussed here is of the beri distribution just seen

00:27:47.360 - 00:28:35.720 | Loss function output
so the output of this kind of loss function so this loss function should have some knowledge of the ground ruths Y and we are going to obtain a scalar number of the loss for and then the scalar is going to be effectively fed into a block that it will calculate the values of the gradient of the loss with respect to the parameter Vector W and this is part of the stochastic gr descent algorithm we have seen

00:28:36.320 - 00:29:16.279 | Parameter update
and the output of this block is going to be called the param parameter update that we will accept a learning rate that we actually call EA this will also accept as a high parameter the mini batch that we have actually called MB and it will the parameter update formula will provide for us and will update for us the vector W of all the parameters involved inside this predictor

00:29:16.960 - 00:29:31.880 | Training and optimization
so this is a very generic block diagram that we allow us to train and therefore optimize any machine Any prediction machine we have seen up to this point


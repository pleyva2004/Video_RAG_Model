in an earli video we have seen the convolutional networks and the basic operation in this video what we actually introducing here is residual networks which is to this day several years after the introduction remain one of the main used architectures for feature extraction and not only as a basic component of many more advanced CNN architectures and that are doing more complicated tasks such as object detection semantic segmentation and others that we will see in another in another video so the history if you like of their introduction you know started around 2015 where people realized that it's not really possible to extend the so-called architectures of the time let's say the vgg architecture we've seen the V architecture on a different U video earlier in this in this actually topic over here where we have SE we have seen the sort of architecture of the V16 Network and what was actually happening then and now that we understand a couple of things about back propagation the gradient had a lot of problems to flow all the way to the input of the network and the U sort of bottlenecks that were actually generated created a significant problems in the training of these architectures so 2015 a researcher at Microsoft you know found a solution on how to enable that gradient to flow freely in a much deeper architectures such as the ones that we will see in a moment and the this gave their the name residual because it in that architecture we Implement what we call a residual unit and from now on we'll refer to those networks as rest nets all right so in order for us to understand what is going on with rest Nets or what I'm going to do now is I'm going to draw a very small rest architecture just consisting of three units U and if I may draw the architecture will that's my unit which I'll abstract with the letter F1 so the input to the so I'll use a bit different terminology from what I was used kind of earlier so I'll be calling set of X I'll be calling it y0 I could have used it also x0 but in my not over here I have the this as y z okay so the y z goes into a block that will consist of one or more convolutional kind of layers and in the residual architecture and this is why we call it residuals we take the input and add it with a unit gain into to the output to the output okay so to form what we call now a y1 the y1 is being added into again with exactly the same approach and this is the output Y2 and the Y2 is similarly added to form the final Y3 output okay so this is the kind of a basic rest net kind of architecture in U and if you compare what we have seen earlier with the cnns we had convolutional layers Max pool layers and so nonlinear evidently over here but we never had this kind of skip over connection as we call it okay so I want to just go ahead and write now expression of the output of each of these blocks with respect to the input so I hope you agree that this is exactly what each of these blocks is Implement okay so we have the FI of Y IUS one plus the Y IUS one in each of these so if I may write down these equations for let's say the Y3 is equal to F3 of Y2 + Y2 the Y2 itself is FS2 of y1 + y1 and the y1 is fub1 of y0 + y0 these are the three equations for each of the three blocks that I have here and what I want to do now is I want to start replacing the Y2 and y1 into the equation Y3 because I want to write down the equation the form of the Y3 as a function of only the y0 and the two functions that are involved in the blocks F1 and F2 okay so all right so what I'm going to do now is I'm going to write it as F3 of Y2 I'm sorry F3 of FS2 of y1 + y1 + FS2 of Y y 1 + y1 okay so I just replace it Y2 with it equal and now I can replace now let me write it over here because I just need a long line to replace it to make the final replacement so it F3 of F2 now I'm going to replace the F y1 with its equal to obtain the expression and this is now the second bracket okay so it is really this bracket over here plus I have FS2 of FS1 of y0 + y0 plus fub1 of y0 + y z okay so this is my final expression with respect to the output of Y3 and why I did that I want to take this kind of architecture and write its equivalent that we just based on this equation and that kind of re- plotting or redraw drawing of this kind of architecture will help me kind of understand a couple of things about the advantages of rest Nets and why they solve the problem of gradient flow throughout the network okay so I am going to start so I'm dividing this into two parts I am going to first draw the long part over here this F3 expression here at the at the bottom so I am going to take this accepts as input y0 that's the only input in this diagram so y0 is going into the F1 we are adding now the function the y z into it okay so that's basically this term all right so then we take the FS2 of this term so the output of this is being fed to F2 and what we do is we add for this FS2 we add this term over here F1 y 0 + y0 so we add to it block involving the function F1 okay so this is basically this inner term over here and then finally we are taking the F3 of that so this is basically my F3 so if I am a Circle if I may Circle this will be let's say a this a will be available over here okay so we have now finished with the plotting of the first term and now let's look at the second term so the second term is simply FS2 of y1 y 0 + y0 so I am going to go again here this is in fact just one line so I'm going to just take again fub1 of y0 + y0 and I'm simply going to take the F2 of that and this thing over here is the point B however this so this is basically B let me throw it over here to B now what we have is we simply add another of these guys we add to be this and I would like to Circle that over because we see here three things being involved a b and c so this is C and then B and C are added to a and this will actually be my Y3 and now that we have this kind of diagram we actually can make some really nice conclusions out of it as you can see the gradient in the backward pass so the point number one I want to mention is about the gradient flow so you can see the gradient flow in the backward pass during back propagation now has a diverse set of paths and to actually flow all the way to the input let's say it has this path that simply just follows all the way to the input y z the other path that goes through this F1 to go to the input this path through F2 and F the concatenation F2 and fub1 or via this skip connection to go to F0 and so on and so on so what we see here is we have a diverse set of paths through of s of gates I will call it because this is what we have used as a term from back propagation of varing depth okay for varing depth so that's kind of important earlier what we had without those kind of skip connections we had simply F1 con with two3 in the So-Cal let's say V architecture so here we had a back propagation that it was involving just a single trajectory a single path through all of these Gates and at some point the gradient was actually dying and of course a dying gradient means that specific functionality of my network they are being updated the parameters are being updated very slowly or just simply seize to be updated so that's not really I mean this is one of the key observations that led to some kind of leveling of the performance of these earlier kind of architectures as the number of layers were being added up in this kind of architecture we are effectively Implement what is actually known as Highway networks and those Highway highways that we creating for the gradient empirically has shown that we are actually can go much deeper so we'll see now some depths typical depths that we experience in we have in a resent architect in a moment the second aspect of that is a bit more nuanced and it has to do with what we call an ensample learning so in this kind of Ensemble learning architecture what we see is we see U the concatenation the combination of three predictors here three main prediction architectures each of those predictors has a varying kind of functionality so we see a fairly involved predictor which we call a we see another predictor which call B and another predictor that we call C and what we see at the output are the kind of combination of those simply I mean if you are familiar with ample kind of methods which I'll provide some kind of background in a moment we are adding the individual prediction results given the input y z at the to obtain our final prediction output the so-called Y3 hat okay and so ample methods have proven in the field to be a very powerful approach in solving you know complex kind of tasks and in fact some methods are being used both for structure and unstructured data and in the structure kind of data field you have methods such as gradient boosting and so on providing some real state-of-the-art results today so a few words about ample methods is arguably a parenthesis but I think it's a kind worthwhile sort of discussing a little bit about ensample methods so there are various asample methods but I think a common denominator for many of them is that the prediction why hat that we get from the so-called ensample also known as committee methods where we for form a committee a group of experts or weak Learners as we call them is let me call it why hat committee is simply the average let me call it 1 / capital K of the summation from small letter K is equal to 1 to capital K where capital K is the number of predictors that we have here we had three in the rest net architecture of Y hat subk so the premise of emble method is that we don't necessarily to have the single server bullet that will solve the very complicated kind of task of us that we have in front of us but a number of what we call the so-called weak predictors that it will U not perform individually very well but on aggregate they will actually perform much better that and that is really the premise of that and you know one parallel architecture with have in the earlier method that we call the rest net is kind of resembles that kind of architecture because we have some kind of a summation combination of these weak predictions the so-called abs and C's that I have explained earlier so the in Sample methods in general we have we can consider performance wise to consist of in somewhere in between two bounds so the lower bound the So-Cal lower performance bound is obtained evidently when you're you have very correlated predictions and if you are not able to randomize the operation of each of these predictors somehow we are going to exhibit this kind of lower bound where either you form a committee or not you get exactly the same performance it's just like the analogy or the equivalent analogy I would like to sort of share sh is you have a committee of let's say a human Committee of human experts but each expert went to exactly the same school studied exactly the same field had exactly the same University professors and they are actually now called to solve the problem and guess what each one of them is actually offering exactly the same view well that's basically where the point where you experiencing a lower performance bound and the upper performance bound is a bit more nuanced a bit more complicated to kind of understand but the best performance what we can actually get I think it's better understood with an analogy you don't expect every Committee Member to not make mistakes they will make mistakes but what you want to do is to have a committee that they don't make the same mistake at the same time so the uncorrelated errors are involved in sort of show showing some kind of a performance bound that is kind of outside of the scope of this course but I think it's worthwhile providing some kind of guidance as to where and how we will be able to achieve that upper bound so the main three ways that we can achieve this kind of upper bound or try to achieve the best possible performance out of ample methods the first is the data component can we provide in some way different data to different of to the various kind of weak predictors that we have here so that we do not cause exactly the same conclusion for each one of them so the second is to somehow randomize their operation randomization is the second kind of approach there and I can actually can offer an example of randomization maybe we can actually offer a different set of hyperparameters sort of picked by some kind of distribution in this architecture see over here in this course u in some other approaches where we have let's say decision trees involved in these predictors again for structure data I'm referring to then we can randomize their operation by picking different features that they split their sort of trees and there are so many approaches that are you know I guess too many to quote over here but the third approach is a bit more relevant in this specific rest not architecture is to Simply use different predictors or weak learners so the weak Learners that we called over here are involved in the rest net architecture and so here we have a predictor of some complexity we have here another predictor larger complexity and yet another predictor or even larger complexity we have effectively implementing you know the third approach where we have those different week Learners each one is offering a view and then finally the network is deciding based on the composition of those views okay so that has been shown to sort of provide performance advantages and that's what we kind of had this discussion about and Sample methods and the third kind of Advantage I wanted to quote here for rest n is there scalability so the scalability should be understood from the point of view of complexity we are effectively able to have three six n or whatever number of residual blocks each one of them will actually be exactly the same as you know any other block over here and therefore we are able to architectures that are have various number of these blocks let's say we see resets with 18 layers 34 layers you know 50 layers 150 102 layers even 150 layers these are the numbers that we have we have defined already existing architectures and this is kind of important when you have perception systems that need to comply to some realtime latency requirement evidently the larger the number of layers you have the longer the latencies that you are going to experience taking an image through this kind of pipeline so if we have let's say a latency of let's say MCS we can and therefore we are not able to accommodate 102 layers where definitely going to be accommodating let's say 50 layers and the exactly the same technology exactly the same thinking and behavior of rest Nets will be in either of the numbers quoted here in terms of number of layers so all of these three advantages are coming together to provide a fairly robust architecture has actually proven in the field in both real time and unreal time applications and able to extract features provide if you like representations on visual imagery that we have the imageries that we are feeding into 
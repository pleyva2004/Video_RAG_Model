In this example, the notebook is quite instructive because it refers to a small data set and I think working with small data sets are actually handy in the beginning when you are trying to understand what is going on. Here we have the classic case of dogs versus cats. We have also the simplest possible task in machine learning which is classification, image classification in this case, and we are going to be using convolutional layers in order to classify the presence of a dog or a cat in an image.
The dataset is available in Kaggle. The original dataset contained 25,000 images but we have cut down to 1,000 images per class. We have split the dataset into train, validation, and test datasets. We are going to use train and validation to create our model and of course, we are going to exercise some kind of prediction API using our test dataset after a model is created.
The architecture we're going to be using here is an architecture that we have developed specifically for this example. It consists evidently of convolutional and interleaved with Max pooling layers. You probably recognize the API here; in this case, it is a kind of a Keras API. Similar architectures can be developed for PyTorch.
The first layer over here is a convolutional layer. The input images are of 150 by 150 pixels. This is what the images that we have transformed now are, and each image is a naturally colored image of three channels: red, green, and blue. We use 3x3 kernels and the 32 here indicates the number of filters. We are going to be using a rectified linear unit, the same nonlinearity that we have used in the fully connected layers.
Then we are passing the output feature map produced here, and by the way, this is where you can actually see the usage of that kind of formula which I was pointing out regarding the output feature map dimensions in an earlier video. The Max pooling layer in this case is 2x2 and it will further shrink the output feature map produced by the first layer, selecting the most important features out of it, passing it over to a convolutional layer with 64 filters.
Here you see now the pattern of increasing the number of filters as the network becomes deeper and deeper. At some point, after one, two, three, four layers, four convolutional layers, we are going to have the head. It's worthwhile going back into this VGG kind of architecture and look exactly where that head was in that architecture and couple it with this code.
The head in this case is a concatenation of fully connected layers. Why we have this kind of concatenation and not just a single layer is because, gradually, even within the head, we need to gradually reach this point of desired number of classes. We have a classification use case here; this is a thousand classes that need to be present at the top of the network, and this is basically the dimensionality of our posterior probability distribution.
We're going to have the y-hat, which consists of a thousand numbers, also the number of classes in the ImageNet dataset. This dimension corresponds to the ImageNet classifier dataset. So that's basically our head, as also seen over here in this code with this portion of the model.
We have whatever we have produced in terms of convolutions over here, and then we flatten the network. By flattening the output feature map, we are creating effectively a volume. We're taking a volume at the input and we're flattening it into a vector. This vector then is passed to two dense layers. The first dense layer has 512 neurons; it takes whatever dimensionality the flatten layer provided and reduces that, just like any fully connected layer we have seen in a corresponding video earlier, into 512 and we use the rectified linear unit for that.
Then the subsequent layer takes 512 dimensions and reduces it further into a single dimension because, as we have seen in the binary classification, we have a binary classification use case here, either we're going to have a cat or a dog. We have just a scalar that we need because that is the probability of the positive class, probably the dogs here, and we are of course going to be using sigmoid because only at the output of the sigmoid we are actually getting this form of the posterior probability as we had discussed in the fully connected layers and in that lecture.
This is basically our architecture: very simple architecture, the convolution portions, the flatten, and the fully connected or dense portion to provide the binary classification result at the output. Here are the details of our CNN. We can see the input images that are actually coming in; the first we have 32 filters as we discussed in terms of number of parameters: 896, 18,000, 73,000, 147,000. So all of these are parameters that you see being quoted here next to the convolutional layers.
But the most striking thing over here is this: look at the number of parameters which are involved in the fully connected or dense layer, 3.2 million parameters. So out of the total 3.82 million parameters that we have, 3.2 million are associated with a fully connected layer. Here is the kind of striking example of why it would make sense to actually use CNN for image classification. If we didn't have the CNN and the associated advantage that CNNs provide, which was actually also shown in this kind of snapshot architecture, as you can see only the local pixels, the ones which are local to the spatial dimensions of the filter, are so-called firing in order to produce that kind of scalar, as compared to a fully connected architecture where everything that we have here is going to be connected to the layer to form, if you like, the output scalar z. The convolutions operation is actually helping us to significantly reduce the number of parameters.
So at the end of the day, we have the scalar that indicates the posterior probability of the positive class as we discussed, and then the architecture seems to be valid. We are going to evidently use binary cross-entropy, just like what we have done earlier in that other video where we looked at dense layers only for binary classification or classification. And here, well, the author selected the RMSprop, which is one of the cousins of stochastic gradient descent. We haven't really got any discussion specifically on enhancements of stochastic gradient descent, but if you do replace it with SGD, I think you will be getting very similar performance with the corresponding learning parameter. And then, of course, the metric is our accuracy.
One of the things that we would like to point out in this kind of convolutional networks is that we will need to be careful when we first take a dataset and we try to process the images. As we have seen, the images are typically given to us with pixels corresponding to integer numbers, so we have to definitely normalize them, we have to batch them, we have to do a lot of these kinds of transformations in order for us to produce the right inputs for our network.
After a training process that involves multiple epochs, as we would expect, we have a model and we can actually plot the training and validation loss as well as the corresponding kind of accuracy and look at the corresponding loss over here plot as the number of epochs. Remember what we have said in another video regarding the condition of overfitting, and at that time the discussion was an example of a linear model on a regression task. Over here we have a classification task, but the sort of problem of overfitting is present across tasks in machine learning. We see some quite significant difference between training and validation as the accuracy improving, and that is really what we have said earlier as a good indicator of overfitting.
It seems that the network that we have designed over here overfits the dataset we are given, and it shouldn't be a complete surprise to us given the fact that we are throwing a significant number of parameters in a network in a dataset which only has a thousand labels per class. So we can actually engage any of the techniques that we have seen in overfitting to address overfitting, such as weight decay, any of the regularization techniques that we have seen also in neural networks to address it. But in computer vision, we have something else that could actually help us, and this is actually called data augmentation.
I think it's worthwhile going through data augmentation because it is really a fairly straightforward and widely used approach to avoid the situation such as this where we have overfitting. In data augmentation, what we actually do is we are taking the input images and given the fact that we have the knowledge of the class, we try to transform these input images, creating more data. That's an artificial way of increasing the number of labels we have in our dataset. We have various kinds of transformations: we may be shifting, rotating images, we may be shearing the image, we are zooming in, zooming out, and flipping, and so on. We are definitely going to be creating some nasty cats or dogs, but definitely, this helps our network to not overfit.
So if you are to just keep the exactly the same network architecture as we have seen earlier, not touch at all the model but definitely train the model with this additional kind of dataset, then look what happened. We have a training and validation loss which are very close to each other, so we actually have solved the overfitting problem, and our accuracy is both in terms of training and validation are also very close, close to something like 85%. Okay, so I think this is a good example to showcase the CNN models as working for the simple task of image classification.
And what we actually also would like to understand now next is what we have said earlier about how can we have some kind of visualization into the internals of the CNN to understand what is actually learning, and this is what we will be discussing next.

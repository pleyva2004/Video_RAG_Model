00:00:03.840 - 00:01:45.159 : we are in the trajectory where we are going to obtain a Blog diagram of our first classifier binary classifier that is but before we do so I think it's worthwhile kind of thinking a bit about the two general Frameworks which are present in the design of this probabilistic models that will give us the functional form of this binary classifier so in general there are two Frameworks the first one both of these Frameworks involve the posterior probability but in a kind of a different way so the first framework is called discriminative and the second one is generate and the difference between the two is quite important and they are although they both effectively are modeling in a different way at the posterior let me call this posterior y of the class small letter K given X which as we have seen in the probability review section this is the x given YK * P of YK and divided by P ofx so this is the posterior and these two Frameworks as we will see kind of model it a bit differently
00:01:46.159 - 00:02:44.360 : so the discriminative framework we will be discussing in extensively in this binary classifier block diagram is the one that will involve Direct modeling of the posterior so the discriminative I call it the first framework and the second framework so the first framework so methods model the posterior directly so we get this posterior from the block diagram itself and while the generative ones are effectively model the posterior in Parts in its from each kind of components
00:02:45.319 - 00:03:32.760 : so we'll first deal with the so-called discriminative classifiers and I want to connect the earlier discussion we had about the radar problem in that kind of video we have introduced a problem where we had we actually went and Drew all the areas under the two probability distributions that were gave raise to the probability of mistake so I want to flip the coin now and actually H sort of discuss the probability of being correct not the misclassification error but the U when we when we have the so-called true positive events and trying to maximiz imize them instead of trying to minimize them is classification error
00:03:33.760 - 00:06:27.000 : so I want to kind of come up with some reasonably intuitive answer to the following question which I'm writing over here why the importance on the posterior P of YK X and how this is related to our performance metrix so if I kind of repeat this kind of discussion but from as I said from the probability of being correct in this kind of classification problem we have again this kind of two integrals but these two integrals now capture the correct events this is when we have probability of Y is equal to 0 DX plus another integral R1 probability of X comma y = to 1 DX these are effectively the flipped areas that from the ones we have actually Drew if you want going to go ahead and review that kind of video that will actually be helpful which is of course equal to a summation in general for this is now the general case where we have Capital One to capital K in this specific capital K is equal to two but this formula that I'm actually writing here is going to be general for capital K classes or of the integral over the RK of the so I'm replacing the joint with the posterior times the marginal and now it's actually a bit more evident how maximizing P effectively involves maximizing the posterior because this term over here is independent of the assignment of to the YK
00:06:28.000 - 00:10:47.440 : so now we have connected the direct connection effectively that of the probability of being correct and the maximization trying to maximize the probability of being correct effectively means maximizing the posterior probability so let me write that down because it's kind of maximizing P correct is equivalent to maximizing P of y k YK given X effectively this points to the following if we are to plot the distribution of the posterior actually we will see here we'll see something like that let's plot the posterior probility distribution we went from we went from distributions at such as this if you remember back in the U in the discussion of the binary classifier something like that we have seen of X and this is the probability of X comma y this was the probability of X comma Y is equal to Zer and this a probability of X comma y isal to 1 and if we are to plot the posterior probability distribution we will be coming up with something that it will look like this in general this is a very general kind of plot so this is one to make sure that we do not exceed the one probability of one so this is probability of Y is equal to 0 given X and this is the probability of Y is = to 1 given X and so the histograms that we have actually the histograms that the posterior hisrs actually have come up with for a given x0 that is coming to us as a let's say a new X that we would like to classify as positive or negative let's say x new that we have never seen before touches these two Curves in this kind of two points I actually we can actually see here that these two points correspond to the discrete so this is this is the for the zeroth class and this is for the let's say class one this is the probability Mass function of the posterior distribution at the output of our predictor so this is the P of Y is equal 0 given X new and this the probability of Y is equal to 1 given X new and so what we have just recognized over here is that all we have to do is we always pick the Pro the class that gives us the maximum posterior probability output and rest assured if we do that we maximizing the probability of being correct so this discussion kind of resulted into this kind of intuitive conclusion but it was not really evident initially how the posteriors and the probability of being correct are related
00:10:48.440 - 00:26:40.559 : continuing now for the discussion we just had on the discriminative kind of classifiers I directly model the predict the posterior we can actually write the posterior probability as follows the Y let's say is equal to one given X is the probability of x given Y is = to 1 time the probability of Y is = to 1 divided by the probability of X but we will write this probability of x given we have let's say two classes or as the probability of Y is equal to zero probability of x given y to 0 * probability of Y is equal to 0 we are using here the sum rule of probability that we have reviewed plus the probability of x given Y is = to 1 * the probability of Y is equal 1 so if you divide both terms by the if we divide both terms with a pro with the numerator we will come up with the following expression 1 / 1+ the probability of x given Y is equal to 1 probability of Y is equal to 1 * ided by probability of x given Y is equal to 0 probability of Y is equal to 0 to the minus1 and this is now related to the probability of odds because the probability of odds the not the probability was the odds if we now write down the odds is the ratio of this divided by of divided by this so for example in a horse race where we have a horse that runs 100 races and wins 25 times and loses the other 75 times the probability of winning is 25 over 100 that's well known to us 25% but the odds are 25 over 75 or 33% so or one win to three losses so this is what we have actually defined over here in terms of our odds the probability of winning to the probability of losing so that's effectively if we can assign this to a positive number if we assign model it as a posst number typically we use the e to the power of some kind of positive number a to model that then we can and you know have effectively these two expressions and from these two expressions we can actually write now the form of the posterior probability distribution so that is the probability of Y is equal to 1 given X is 1/ 1 + cus a and this is a wellknown function that is actually called the sigmoid function because it is when we actually plot this function it will look something like that so over here will be 0.5 and over here will be the one and it will look like a sigmoid that will give will take as input a and will provide Sigma of a and all the output is going to be constrained between 0 and one so we have effectively came up with this kind of expression of the sort of a pro posterior probability distribution at the output of a sigmoidal unit with having as argument some kind of input a now if and this is kind of motivates the kind of logistic regression if a is a linear combination is a linear combination of features let's say a is W transpose f of x we have seen both of them notations in our linear regression kind of then this model of the posterior is regression which is a wellknown and fairly popular way to do binary classification so effectively over here we have the assigned the to the log ODS so another way actually of seeing it is that if we take here the log of odds in other words the log of the probability of X comma Y is = 1 divided by the probability of X comma Y is equal to 0 so this is going to be effectively a and if this a is equal to W transpose f of x this is the form of U logistic regression so the logistic regression is followed by the is actually implemented using the following diagram as it's actually indicated here first form a linear combination of features and this is the dot product in other words between the feature vector and the parameters of the of our model W and then pass that through a sigmoidal unit in order to obtain a posterior probability at the output so we kind of obtain the logistic regression kind of block diagram for kind of a first principle so the block diagram is going to be W transpose f of x where we have taken X and very similar to what we have seen in logistic regression we went through a featu riser to obtain I of X which we have used in this kind of dot product to form a scalar a and this scalar a is at the input of a sigmoidal unit Sigma and that y hat rest assur is going to be the probability of Y is equal to 1 given X so this is our first classifier that we will be calling a generalized linear model and we call it generalized because of the nonlinear unit which is definitely nonlinear because the every sigmoidal unit can take any number from minus one million let's say to plus one million but it compresses that into a dynamic R between 0 and one we definitely want the output to be 0 and 1 because we have interpreted the output as a pro as a posterior probability but definitely there a nonlinear unit so this kind of long Nan transformation from a to the posterior and that's why we call it generalized but definitely it's a linear model in a sense that it is you know one of the blocks of the diagram involves a linear unit a linear combination of the features FX so all we need to do now is to attach to it two things the first is the binary cross entropy loss which will accept also the ground truth y this binary cross entropy loss will be feeding the well known to us stochastic graded descent kind of algorithm with it's kind of parameter gradient calculation and parameter update will feed the W and will update the W at every iteration so this block diagram is no surprise To Us by now we have seen it so many times in both linear regression and now classification and the only thing that remains to be done over here is to come up with a expression of the Cross entropy loss the binary cross entropy loss with respect to the set of parameters W and this can be shown to be U some form such as thisal to 1 to M of Y IUS y I Pi of XI so now we have some expression about the gradient that we need in order for us to implement sastic grade descent and now we will see in a notebook how the stoas r descent is powering logistic regressor and in fact it's Al also worthwhile commenting on how the name kind of logistic regression came to be attached to this kind of block diagram and in fact if we treat this binary classification problem like a regression problem and we plot over here the U so-called XIs versus the Y similarly what we have seen in so many regression problems then definitely r y is discrete random variable and take values between zero let's say and one and certainly for in this kind of neighborhood we'll see many assignments to zero and in this neighborhood remember the radar problem High signal strength low signal strength High signal strength mostly we will get a positive prediction of our attacks and over here we're going to have a negative prediction of our attacks here we're actually plotting here the ground truths and if we are to if we are to do regression to fit this data in a very similar way as we have done earlier probably will come up with a kind of a straight line that tries to maximize some objective kind of function so this is straight line regression is not going to be very appropriate for a classification problem because we are expecting our predictor to produce values always between zero and one so therefore what we do here is we are applying the sigmoidal so this is effectively the line that generates the a when we have no features no featurization in this specific Syle example and so the sigmoidal unit will match effectively the it has a linear component over here and will compress everything between zero and one so that's another way of kind of graphically remembering logistic regression as an attempt to do regression but at the same time with a kind of a compressive step okay so now what we will do to conclude a little bit the topic of classification is to just in passing quote a couple of things about the second framework that I have mentioned the soal generative classification Frameworks in the generative classification framework we're again going to be task to calculate the posterior probability that we see here for in general kind of K glasses and this posterior is going to be evidently given by this General kind formula so in the generative approach we will do two steps instead of coming up with the block diagram that generates that from directly and models the posterior directly as we have done with logistic aggression we will first do two steps one is to estimate the likelihood and the second is to estimate marginal and then come to some degree of approximation because it's actually typically a very expensive for large dimensions for Gen this is a very expensive calculation so for we will typically involve some form of approximation for calculating this denominator over here of the posterior the knife base is a famous generative classification method and we actually going to see that when we come to language modeling and some other tasks later on in some videos so I will take a rain check to discuss it at that moment and revisit if you like the generative classification framework and the discussion of on Bas is not really essential right now for us to progress

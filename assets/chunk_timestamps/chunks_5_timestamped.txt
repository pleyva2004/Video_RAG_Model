00:00:00.480 - 00:00:26.279 : In an earlier video, we saw the structure of the convolutional neuron and how many of these neurons in the form of filters are, you know, coming together to form a layer and how the multiple layers are coming again together stacked to implement this binary classification task in this video that we have looked at on cats versus dogs.
00:00:27.279 - 00:00:46.360 : Now we will be using exactly the same model that we have built for that kind of task and in this specific case, what we were interested now to see is to validate what we have said earlier about the some kind of a structure or pattern that we see in the features that the convolutional networks kind of learn.
00:00:46.520 - 00:01:12.640 : So in this notebook borrowed from the book Deep Learning with Python, what we actually can see a couple of things. The first is the so-called U, the convolution network outputs. These are effectively what are each layer kind of presents to the layer above it and I think it's worthwhile kind of going through that first.
00:01:13.320 - 00:01:29.680 : As we said, this is the sort of exactly the same architecture we have seen earlier for that specific data set and this is the input images, the 100 approximately 150 by 150 pixels and natural Rec col images.
00:01:29.920 - 00:01:58.479 : This is the, you know, first layer what it really learns and as you can see the output of the output kind of feature map it kind of presents an almost identical figure to the sort of input picture input image except that this image over here emphasizes the edges.
00:01:58.799 - 00:02:19.760 : These are the, as we said, the primitive kind of shapes that the first initial layers of the conet are actually learning and we can actually go and look at each and every layer and I think the over here I think this IM figure over here shows what is really happening.
00:02:20.720 - 00:02:49.239 : The initial layers are learning, I will call it, a visual content, the same kind of visual content as our eyes kind of see in the image. Let's say you can very clearly see the shape and form of the of the cut over here but as we actually going further deeper into the network, then the representations that are actually being created are becoming more and more abstract.
00:02:49.760 - 00:03:09.319 : To the point where this is the fifth layer, as you can see from that point onwards, we still see some of the feature maps that are being created. Remember, the feature maps that are being created are volumes so what actually we see here are the flattened version of those volumes.
00:03:10.159 - 00:03:30.680 : So here you have the sixth layer, the seventh layer as you can see here we from the seventh layer onwards we are not really able to see any of the sort of visual characteristics of a cut so this become a fairly abstract kind of representation.
00:03:30.799 - 00:03:58.280 : Here you can actually also see very clearly the impact of Max pooling and how we can actually start with a representation and what is the max pooling operator with a cal of 2 2x two is actually doing is actually picking at the more essential kind of in features that are presented to it.
00:03:58.519 - 00:04:25.400 : So if you compare this image and this image and so that's effectively at this point we have the representations that are going to be needed after the stochastic rated descent kind of converges and provided we don't have any over fitting and so on these representations are the ones that we are going to be flattening to and then feed them into the fully connected layers that constitute our head.
00:04:25.639 - 00:04:50.400 : And if everything goes okay, this head will actually see and work on those representations to actually do the binary classification. Okay, so this is what we have seen we can actually see in the feature maps and I think it's also worthwhile understanding what we actually see now in as far as the filters.
00:04:50.880 - 00:05:22.639 : What really the filters, the contents of those filters are to visualize those filters what we actually do is we define a specific loss function, the details are kind of outside of this course of the scope of this course but the at a high level what we do is we try to find input images that maximize the activations that those filters produce and therefore in the process of doing so we are able to out of this optimization process to retrieve those filter values.
00:05:22.840 - 00:05:58.440 : So here we see just the first 64 filters out of the many more that we have used I think we used all the way up to 256 filters but we here we see the first 64 filters in a 8 by8 kind of pattern and as you can actually see the U this filter over here is for the various kind of layers so these are effectively we have the layers going from the beginning of this fil of the beginning of the network all the way to the U towards the just before the head of this.
00:05:59.840 - 00:07:50.080 : So this is the last layer just before the head and as you can actually see here in every layer we learn effectively a collection of these filters that it will decompose the input image the input the input feature maps are being decomposed so if you go back to what we have discussed earlier about the operation of the convolutional kind of neuron so imagine that you have now not only just one filter but you have let's say 64 of them so the each one of those filters is one component out of the let's say 64 that the input feature map will be decomposed so you can read about this as those are the components of that decomposition so this so in the last kind of layer over here we have 64 components for those who have some background on principal component analysis there's some something to it along those lines but the composition over here is definitely exactly the same as any other decomposition it's just that these components as the layers are becoming deeper and deeper are these components are you know look quite different and this as you can see here the filters are simpler in the first layers and become more and more complicated in the subsequent kind of layers to match the sort of nonv visual intuitively visual complexities that we have seen in the output activation Maps or feature Maps that we have seen earlier.

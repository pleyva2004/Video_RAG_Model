00:00:01.040 - 00:00:46.800 : so this is basically what is happening in a an example over here where we have an input image and we are actually sliding a kernel a 3X3 kernel and we are getting an output feature map so the input feature map here is just has one we will be calling sometimes this depth Channel and the output feature map has again one channel over here and what we have here is we have a couple of things that we need to introduce as terms in convolutional operations that we actually doing and inside the convolutional new networks
00:00:47.640 - 00:02:07.360 : so the first is the form of the concept of padding and typically we are padding the U input feature Maps in order to do two things we achieving two things probably you have noticed that in an earlier kind of discussion or we had the operation of cross correlation operation in this in this kind of image over here the output feature map was always smaller in terms of speciaal dimensions compared to the input feature map and it is evidently so because the only way that this output feature MK can be exactly the same size as the input is when the kernel is 1 by one so when the Kel has a special extent of 1 by one then we have exactly that situation but in most cases where the can won't be one by one we will expect this output feature maps to shrink in terms of spatial content and we do not want them to shrink too much because sooner or later we will be running out of spatial Dimensions in our outputs and therefore we cannot really go deep to construct deep architectures in these networks
00:02:08.119 - 00:02:54.319 : so what we expect to do what we are have done is we have with padding we are trying to manage this special extent reduction on one hand as you can see if we had this padding over here then the output feature map is going to be much larger than otherwise so if you can imagine that without a padding then this sort of output feature map would actually be I can't really sort of tell you exactly the dimensions but if you do the if you see the sort of if you do it visually then you can actually see it's going to be probably something like a 3X3 output
00:02:56.280 - 00:04:11.920 : now the sort of another advantage of padding is that we allow the Kel to actually move into locations which would not be able to move otherwise so a kernel as we discussed a bit earlier contains some values and we would like all of the pixels including the edge pixels of the input feature map to be able to be correlated with all of the other pixels of the all of the of the pixels of the Kel and therefore padding allows us to do so otherwise you can imagine this red kernel over here will actually be only be able to correlate with those three pixels of the input feature map now this pixel over here can be correlated with both this pixel of the kernel and that pixel of the and this pixel and this pixel of the sort of Kel that we have so we have the ability to sort of get more information especially towards the edges of that input feature map with padding
00:04:12.920 - 00:05:35.919 : another parameter that we should also sort of understand is this kind of stride so stride is the just like the stride that you as you walk it this here actually refers to the number of pixels that you are skipping over the in order for you to be able to do the next correlation so here you see two locations of that kernel in that location and the blue location the red location the blue location if your stride was one then the blue K have been right here and while with a stride of two then we don't get one correlation operation for every pixel of the input feature map and this is obviously is helping us to manage the complexity of these filters in fact goes slightly to the opposite direction of what we have said earlier in a sense that in some instances we prefer to get for some of the layers of the convolutional neuron the The Stride parameter to be larger than one typically The Stride parameter of height and width are going to be the same so that's what you see over here
00:05:36.639 - 00:06:34.759 : bottom line is that all of these parameters and far more that are to follow are hyper parameters and we are going to be optimizing them for using hyperparameter optimization in order for us to define the complete architecture of a CNN here you see some animations that kind of reinforce what we have just quoted Ed without padding the Kel the output feature map is going to be U potentially significantly reduced in terms of spatial extent something will make any subsequent cor correlation with Kels you know not very useful with padding this is we avoid that and here we actually have padding combinations of padding and stride so I suggest that you study this kind of animations to just get the gist as to what padding and stride are actually offering to us
00:06:35.599 - 00:30:19.240 : but now the time has come to look at the operation of the convolutional neuron Network and in fact the describe if you like the single convolutional kind of layer in detail we will start drawing a snapshot of a CNN layer operation that will actually help us to understand the general case where we have input U and output feature Maps coming into the CNN layer but however these input and output feature Maps possess different depths and this is another parameter that we have to understand you know that the we are responsible for Designing these layers with that the depth of what we will produce is our responsibility to design so let's write now draw if you like a picture of that CNN layer in operation okay let me call it the snapshot we will see just a single snapshot of that layer and this will also help us understand the U what is the convolutional neuron we already have seen the sort of sigmoidal kind of neuron now we will see in the in the fully connected dense layer architectures now we'll see the convolutional neuron in front of us so the snapshot let me call it snapshot of CNN of a CNN operation all right so let's draw now the general case as we discussed that we have an input volume this input volume is associated with h the output feature map of an earlier layer let's call that layer l minus one this is basically the feature map that was generated by the previous layer in general and we'll have a depth of capital m l minus one it will have some kind of width let me make sure that you can see here this is WL minus and the height over here would actually be okay this is a depth and the height over here will actually be h l minus one all right so that's basically the dimensions of my incoming volume and this incoming volume has some kind of resolution in terms of number of height and width pixels let me just draw them quickly because we would to now draw the U what will be the output of out of this operation which is the output feature map now the output is going to be generated at this specific moment in time I have in general a filter that has 3x3 special extent it is located let's say here at this moment in time because that's why you call it a snapshot and it has some depth I want to discuss a little bit the depth what makes sense for this depth of the filter to be but it when it is located over here for sure I'm expecting to have some output feature map this output feature map will be smaller in terms of spal extent that's why I'm kind of drawing it like this it has some kind of a number pixels okay and we have some kind of a depth and this depth is definitely something that I need to control because it's one of my main design parameters I'll be calling this depth ML and evidently we have a different HL and WL dimensions and this is basically my you know volumes input and output volumes in general going to have input and output fors so the question I actually have right now is to understand a little bit about the depth of the filter and we have three options either the depth of the filter will actually be deeper than the input feature map shallower than the input feature map or exactly the same depth as the input feature map so let's try to do some kind of reasoning over here does it make any sense for the filter to be deeper than the input feature map and if you think about it the answer is no it does not really make a lot of sense because at the end of the day we are going to be correlating the contents of that filter with the contents of the input feature map and if the filter is actually deeper then we are not going to be picking up anything from the input feature map because we are going to so why have it deeper okay so you know there's no point of doing so if it is shallower than the input feature map also it does not really make a lot of sense because we are going to leave content that the input feature map U contains for us on the table so the so the only reasonable assumption is this filter to be exactly the same in terms of the input feature map depth right in terms of this terms of depth of the input F map so it's just basically draw it as such and it in fact it is really this filter that is going to be a going to be the one that we are going to be using to do this kind of a three-dimensional kind of a correlation over here now to understand the contents of that correlation is kind of important and what is actually even more important to understand what it will generate as we will see shortly what it will not generate it will not generate the whole volume over here but it will actually generate only one slice out of that output volume okay to understand that kind of important Point let's do the following let me take the sort of so for that specific snapshot that I'm actually right now I'm generating the specific let me drew that like there some specific result which is a scaler therefore it's a result of a single Pixel from this column which is located at the coordinate I comma J so specially wise and I hope you remember what we have seen earlier in the sort of example architecture sorry in the CNN architecture diagram we are let just show you U that kind of diagram again for that specific snapshot let's say the blue La snapshot I'm actually generating this scalar result and using just one kernel a filter of depth one in this case so as it will actually as it actually turns out that fil that filter at that specific snapshot it will do a three-dimensional correlation and it will still generate a single scaler for me okay and that single scalar will be at a specific depth okay and the special coordinates of that scalar is I comma J that the one I just drew now we will call that depth with an index in a moment but what I want to do here is to just draw the complete of pixels at I comma J location let me just rotate them this 90° and write it over here it will be evidently this Dimension will be ml the depth Dimension and this is the because we are correspond to the earth layer and let me just do exactly the same thing with the filter so I'm actually taking the filter and decompose it over here to the 3X3 Kels that it contains and so these are going to be my 3x3 kernels and this will be of Dimension ml minus1 so just took the filter rotated 90° and just decompos into its Kels this is the L minus one layer and so since I'm going to be generating a scaler let's assume that I'm generating right now at that specific snap sort the this is the I comma J coordinate this is the column that corresponds to the E layer and the I comma J coordinate let's assume that I'm generating this scalar over here this scalar is going to be represented by the Z and we'll have evidently I comma J as a special coordinates and we have a depth coordinate which I will designate with the letter KL and evidently 1 is less than or equal to KL is less than or equal to ml and this will actually be the values that the KL index which is the depth index can take and I will actually be using also a corresponding index to address each one of those Kels which are going to be used for the determining that kind of scalar Z so that scalar Z is going to be produced by using all of the Kels of the filter and I am going to also need to Define to Define two other indexes the first index is going to be U and the other index going to be V and this indices will actually be used to as spatial coordinates of the kernel okay so my equation so is the following so given I J comma KL given in other words the coordinates of the scalar which I want to generate my scalar z i comma J comma KL are going to be given by three summations the first two summations I have seen already in the plain two- dimensional correlation operation the one that we just did in an in a earlier so this is U summation over u and v definitely I'm expecting the special content of that kind of filter to be correlated and therefore dot product to take the dot product with the contents of the input image okay so this is the two summations over here but also I'm expecting to now do a three-dimensional correlation operation that's a third summation over an index I'll be calling k l minus1 and this addresses the specific kernel which I'm going to be using so KL minus one is definitely the less than or equal to one and less than or equal to ml minus one in a similar in a similar way as we have seen earlier so what is this kind of a correlation it will be X of I + u j + V kl-1 time W where W now are the contents the of the of the kernel that now has U comma V comma KL comma KL minus1 all right so we have in fact the W is not the cond of the kernel the cond of the kernel yes we can call them W but W I would associate W with this line over here that for specifying this line I have the U comma V coordinates spal coordinates of the kernel that specific kernel however is provided by this index is identified by this index so this specific Kel is by this index and the scalar it is going to be generating is the located at the KL depth that's why I need this W of Q comma V comma KL comma KL minus one okay so this will actually be the weights that are going be so a four dimensional tensor is being used here for specifying those parameters that we are store in those in those filters and in fact we only have one filter right now so a four dimensional tensor to identify the parameters that we have used in this specific dot product over here this is a three-dimensional dot product and as you can imagine as I'm sliding the filter to another location in the next snapshot the only thing actually is changing is the coordinate that is being produced in this scaler so the only thing so by moving the filter around I'm changing the I comma J of what I'm producing therefore what I'm actually going to be is a slice a specific slice out of this sort of output feature map so the specific slice I'm just drawing over here just one of the ml slices so ml slices generates the complete volume so this slice is the one that I am going to be generating this complete slice so effectively a matrix and so from one filter I'll be generating a single Matrix and therefore and this is the important conclusion from we multiple we need multiple filters to generate the feature map volume so for a volume for the whole thing for the output feature map we need to be creating multiple fatures in fact this thing over here is really the connectivity diagram of the convolutional neuron what we call a convolutional a single filter is and the operation actually we see over here is the operation of the convolutional neuron and this is all of these parameters that we have used over here the contents if you like the filter are the so-called trainable parameters and we will now see an animation of this thing in our core site so if I go to my core site and actually scroll down a little bit then you can see now the three-dimensional so first of all before we see the animation we can actually see it's exactly the diagram I just drew a bit a different I have an input volume which is the blue over here with has a depth D in definitely as we mentioned the input sorry the filter that we're going to be using has the same depth D in as the input volume doesn't make otherwise and then in terms of the output volume a single filter is going to be generating one slice out of the D out slices so on one slices let's say this specific Matrix over here where my mouse pointer is that is going to be what is going to be produced by a single filter and this dotted line here indicates that if you want to generate the complete output volume with a depth D out you need D out of these filters you need D out of these orange cubes in order for you to be able to generate a complete green output feature map so I think it's worth spending some time in this animation in this animation you can actually see exactly what I just discussed here in this example I have an input feature map of depth three and I have a output feature map of depth two let's assume that is the sort of a design parameter which I want to implement therefore if I have an output feat map of two I need two filters and these are the two filters this is the filter w0 and this is the filter W1 and evidently the each of these filters has of dep depth three because three is also the depth of the input feature map and at every specific snapshot let's assume this is the snapshot that I just drew on piece of paper this filter is located at this specific location in my input feature map and it is responsible for creating this scalar Z which is nine in this case okay so if I may and as far as the output feature map is concerned this filter is only able to plot if you like to determine the this specific slice of the output feature map if I want to continue then we will see that the second filter is the one which is involved in the creation of the second slice of the output feature this is really the essence of a three-dimensional convolution I suggest that you spend some time on this animation trying to understand what is what is going on and you can toggle the movement just to be able to replicate the output scaler from the input values which have been provided over here a bit on this presentation of the snapshot operation of a layer the site over here has is squatting some kind of important formulas regarding the size the special dimensions of the output feature map I think it's important to note them down and so it is the floor of the height of the input feure map plus two * the padding size minus the kernel size divided by the strides and plus one okay so this is the formula that will that you can actually use to understand exactly what will be your output feature maps are in terms of spatial dimensions and of course this will be the input feature map sizes for the for the layer that follows okay so what will actually be those layers I think it's you know it's quite important to get into the U discussion now about other architectural features before we go into some kind of a discussion about the advantages of convolution layers as compared to fully connected layers which I think is best demonstrated using an example before we go into that example let's look at another operation that we'll be calling the max pulling layer or in general pulling layer which is actually described here and it's best demonstrated with this kind of image and this case what we see we have an input feature map that has a depth of one in this case and we do still have the concept of if you like of a kernel that we slide around just like in the convolutional layer but in this case instead of a nonlinear function like a reu that we have actually also seen in the fully connected layers that we are still going to see in the evolutional layer as we will see in that example we will have another function let's call that function in this specific case it's shown as the max function where the idea behind this is that we are going to not form a correlation result over here like a DOT product but we're going to select the maximum element of what we see in the input feature map typically we apply the that function at for each of the channels of the input feature map but in some instances we may apply it also across the depth Dimension what we are achieving is evidently we are achieving some reduction in the spatial dimensions of the output feature map and that kind of intuitively understood as trying to

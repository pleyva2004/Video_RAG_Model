00:00:00.919 - 00:00:38.680 : In an earlier video, we have seen convolutional networks and the basic operation. In this video, what we are actually introducing here is residual networks which, to this day several years after their introduction, remain one of the main used architectures for feature extraction. Not only as a basic component of many more advanced CNN architectures but also in performing more complicated tasks such as object detection, semantic segmentation, and others that we will see in another video.
00:00:39.200 - 00:01:23.799 : The history of residual networks began around 2015 when researchers realized that it was not possible to extend the so-called architectures of the time, such as the VGG architecture. We've seen the V architecture in a different video earlier, where we discussed the architecture of the V16 Network and the issues it faced. One major issue was that the gradient had a lot of problems flowing all the way to the input of the network, creating significant problems in training these architectures.
00:01:24.000 - 00:01:47.560 : In 2015, a researcher at Microsoft found a solution on how to enable the gradient to flow freely in much deeper architectures. This solution gave the name 'residual' to the architecture because it implements what we call a residual unit. From now on, we'll refer to those networks as ResNets.
00:01:47.759 - 00:02:28.480 : To understand what is going on with ResNets, I will draw a very small ResNet architecture consisting of three units. I'll abstract the first unit with the letter F1. The input, which I'll call Y0, goes into a block that consists of one or more convolutional layers. In the residual architecture, we take the input and add it with unit gain to the output to form what we call Y1. Y1 is then added into again with the same approach to form the output Y2, and similarly, Y2 is added to form the final Y3 output.
00:02:28.800 - 00:03:06.200 : Now, let's write down the expression of the output of each of these blocks with respect to the input. For Y3, the equation is Y3 = F3(Y2) + Y2, Y2 itself is F2(Y1) + Y1, and Y1 is F1(Y0) + Y0. These are the three equations for each of the three blocks. I will now start replacing Y2 and Y1 in the equation for Y3 to write it as a function of only Y0 and the two functions involved in the blocks F1 and F2.
00:03:06.519 - 00:03:53.879 : After replacing, we get Y3 as a function of Y0, which helps us understand the advantages of ResNets and why they solve the problem of gradient flow throughout the network. The diverse set of paths for the gradient flow during back propagation now includes paths that follow all the way to the input, through various combinations of F1, F2, and the skip connections, providing a robust mechanism for training deeper networks.
00:03:56.799 - 00:04:28.520 : ResNets also resemble ensemble learning architectures, where we see the combination of three main prediction architectures, each with varying functionality. This combination at the output, known as Y3 hat, is a powerful approach in solving complex tasks. Ensemble methods, which involve averaging predictions from a group of predictors, have shown to provide significant performance advantages.
00:04:29.160 - 00:04:56.280 : Finally, the scalability of ResNets is another advantage. We can implement architectures with varying numbers of residual blocks, accommodating different real-time latency requirements. This scalability, combined with the robust architecture of ResNets, has proven effective in both real-time and non-real-time applications, capable of extracting features and providing representations on visual imagery.
